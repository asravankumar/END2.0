## Assignment 6 - Sentiment Analysis on Tweets using Encoder Decoder Architecture

### Problem Statement
Using Encoder Decoder Architecture, perform sentiment analysis on tweets dataset.
  - encoder: an RNN/LSTM layer takes the words in a sentence one by one and finally converts them into a single vector. VERY IMPORTANT TO MAKE THIS SINGLE VECTOR this single vector is then sent to another RNN/LSTM that also takes the last prediction as its second input. Then we take the final vector from this Cell and send this final vector to a Linear Layer and make the final prediction. 
  - This is how it will look:
    - embedding
    - word from a sentence +last hidden vector -> encoder -> single vector
    - single vector + last hidden vector -> decoder -> single vector
    - single vector -> FC layer -> Prediction

#### DataSet
  - Dataset consists of 1364 labelled tweets with three sentiments :- negative, neutral & positive.
  - The labels are as follows.
    - 0: Negative
    - 1: Positive
    - 2: Neutral

####  Model And Training
  The network consists of the following:
  - Encoder
    - Encoder is implemented by RNN.
    - The hidden vector of the last state is the single vector.
    - This single vector is passed to the decoder as input.
  - Decoder
    - Decoder is implemented by LSTM.
    - 2 lstms are used.
    - The input is the single vector which is the last hidden vector of the encoder RNN.
    - The first LSTM accepts the single vector which is of shape [batch_size, 1, hidden_dimension]
    - The output of this LSTM is sent as input to the 2nd LSTM.
    - The hidden and cell vector of the first LSTM is passed to the second one.
    - The hidden vector of the 2nd LSTM is then passed to a fully connected layer.
  - Fully Connected Layer
    - A fully connected layer with hidden_dimension as input layer nodes and three as output nodes(one for each label).

  Network:
```
EncoderDecoderClassifier(
  (encoder): Encoder(
    (embedding): Embedding(4651, 300)
    (rnn_layer): RNN(300, 100, batch_first=True)
  )
  (decoder): Decoder(
    (decoder): LSTM(100, 100, batch_first=True)
    (decoder2): LSTM(100, 100, batch_first=True)
  )
  (fc): Linear(in_features=100, out_features=3, bias=True)
)
The model has 1,597,403 trainable parameters
```

#### Encoder Decoder Classifier Classes

  - Encoder

```
class Encoder(nn.Module):
  def __init__(self, vocab_size, embedding_dim, hidden_dim):
    # Encoder class which accepts a sequence(tweet) and converts it into a context vector.
    super().__init__()

    # Embedding layer
    self.embedding = nn.Embedding(vocab_size, embedding_dim)

    # RNN Layer for encoding
    self.rnn_layer = nn.RNN(embedding_dim,
                          hidden_dim,
                          batch_first=True)
    
  def forward(self, text, text_lengths):
    # text = [batch size, sent_length]
    embedded_text         = self.embedding(text)

    # embedded = [batch size, sent_len, emb dim]

    # packed sequence
    packed_embedded_text  = nn.utils.rnn.pack_padded_sequence(embedded_text,
                                                        text_lengths.cpu(),
                                                        batch_first=True)
    # input sequence to the rnn layer 
    encoder_output, hidden   = self.rnn_layer(packed_embedded_text)
    # rnn output in packed format which contains outputs at every sequence.
    # the hidden will be of last state only.
    # hidden = [1 , batch_size, hidden_dim]
    # Note that, the hidden tensor will not be in the batch_first = True shape. Only the output tensor will be in batch_first if it is set to true.

    # unpack the encoder rnn output 
    encoder_output, encoder_output_lengths = nn.utils.rnn.pad_packed_sequence(encoder_output, batch_first=True)
    # will be in batch_first = True shape
    # encoder_output = [batch_size, sent_len, hidden_dim]

    # here returning the output at all states.
    # and the last hidden vector which is the SINGLE context vector for the input sequence. 
    return(encoder_output, hidden)
```

  - Decoder

```
class Decoder(nn.Module):
  def __init__(self, encoder_output_dim, hidden_dim):
    super().__init__()

    # lstm layer as part of decoder.
    # the encoder emits a hidden vector with one sequence. Hence, lstm is here for once in the pipeline.

    self.decoder = nn.LSTM(encoder_output_dim, 
                       hidden_dim,  
                       batch_first=True)
    self.decoder2 = nn.LSTM(hidden_dim, 
                       hidden_dim,  
                       batch_first=True)

  def forward(self, single_vector):
    # Here single_vector is the hidden vector from the encoder.
    # single_vector = [batch_size, 1, hidden_dim]

    # encoder_output = encoder_output.squeeze(0).unsqueeze(1)
    # encoder_output
    output, (hidden_vector, cell_vector) = self.decoder(single_vector)

    # hidden_vector is the decoded vector which is input to the fully connected layer.
    # hidden_vector = [1, batch_size, hidden_dim]
    output2, (hidden_vector2, cell_vector2) = self.decoder(output, (hidden_vector, cell_vector))

    return(output2, hidden_vector2)
```

  - EncoderDecoderClassifier

```
class EncoderDecoderClassifier(nn.Module):
  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
    super(EncoderDecoderClassifier, self).__init__()

    self.encoder = Encoder(vocab_size, embedding_dim, hidden_dim)
    self.decoder = Decoder(hidden_dim, hidden_dim)
    self.fc      = nn.Linear(hidden_dim, output_dim)

  def forward(self, text, text_lengths):
    # Encoder encodes using an rnn.
    encoder_output, encoder_hidden = self.encoder(text, text_lengths)

    # the hidden vector emitted by rnn is not in batch_first=True shape. Hence converting.
    # encoder_hidden = [1, batch_size, hidden_dim]
    encoder_hidden = encoder_hidden.squeeze(0).unsqueeze(1)
    # After reshaping
    # encoder_hidden = [batch_size, 1, hidden_dim]
    
    # Decode the input encoded single vector.
    decoder_output, decoder_hidden = self.decoder(encoder_hidden)

    # the hidden vector emitted by the decoder is in the batch_first=False shape. Hence convert it to shape for linear layer.
    decoder_hidden = decoder_hidden.squeeze(0) #.unsqueeze(1)
    # input to fully connected layer dimension = [batch_size, hidden_dim]
    dense_outputs = self.fc(decoder_hidden)

    # fully connected layer output = [batch_size, output_dim]
    final_output = F.softmax(dense_outputs, dim=0)
    return(final_output, encoder_output, encoder_hidden, decoder_output)
```

Adam Optimizer with crossentropyloss function is used.


Training Logs:

```
Train Loss: 1.095 | Train Acc: 50.34%
	 Val. Loss: 1.092 |  Val. Acc: 57.56% 

	Train Loss: 1.071 | Train Acc: 60.43%
	 Val. Loss: 1.055 |  Val. Acc: 60.98% 

	Train Loss: 1.027 | Train Acc: 66.10%
	 Val. Loss: 1.037 |  Val. Acc: 60.49% 

	Train Loss: 1.003 | Train Acc: 70.93%
	 Val. Loss: 1.040 |  Val. Acc: 61.46% 

	Train Loss: 0.981 | Train Acc: 72.16%
	 Val. Loss: 1.041 |  Val. Acc: 57.07% 

	Train Loss: 0.978 | Train Acc: 74.63%
	 Val. Loss: 1.035 |  Val. Acc: 59.02% 

	Train Loss: 0.958 | Train Acc: 78.86%
	 Val. Loss: 1.031 |  Val. Acc: 62.93% 

	Train Loss: 0.955 | Train Acc: 79.74%
	 Val. Loss: 1.033 |  Val. Acc: 61.95% 

	Train Loss: 0.953 | Train Acc: 78.10%
	 Val. Loss: 1.035 |  Val. Acc: 64.39% 

	Train Loss: 0.946 | Train Acc: 80.09%
	 Val. Loss: 1.033 |  Val. Acc: 63.90% 

	Train Loss: 0.939 | Train Acc: 82.84%
	 Val. Loss: 1.031 |  Val. Acc: 63.90% 

	Train Loss: 0.939 | Train Acc: 81.12%
	 Val. Loss: 1.028 |  Val. Acc: 64.88% 

	Train Loss: 0.938 | Train Acc: 83.88%
	 Val. Loss: 1.030 |  Val. Acc: 66.34% 

	Train Loss: 0.937 | Train Acc: 82.13%
	 Val. Loss: 1.027 |  Val. Acc: 67.32% 

	Train Loss: 0.935 | Train Acc: 84.48%
	 Val. Loss: 1.030 |  Val. Acc: 66.83% 

	Train Loss: 0.930 | Train Acc: 87.41%
	 Val. Loss: 1.029 |  Val. Acc: 66.83% 

	Train Loss: 0.932 | Train Acc: 84.09%
	 Val. Loss: 1.029 |  Val. Acc: 68.78% 

	Train Loss: 0.935 | Train Acc: 84.57%
	 Val. Loss: 1.029 |  Val. Acc: 68.78% 

	Train Loss: 0.938 | Train Acc: 84.72%
	 Val. Loss: 1.027 |  Val. Acc: 67.80% 

	Train Loss: 0.931 | Train Acc: 85.43%
	 Val. Loss: 1.026 |  Val. Acc: 68.29% 

	Train Loss: 0.936 | Train Acc: 83.19%
	 Val. Loss: 1.031 |  Val. Acc: 67.32% 

	Train Loss: 0.935 | Train Acc: 84.48%
	 Val. Loss: 1.030 |  Val. Acc: 67.32% 

	Train Loss: 0.932 | Train Acc: 87.50%
	 Val. Loss: 1.026 |  Val. Acc: 68.78% 

	Train Loss: 0.929 | Train Acc: 88.53%
	 Val. Loss: 1.025 |  Val. Acc: 69.27% 

	Train Loss: 0.928 | Train Acc: 86.64%
	 Val. Loss: 1.026 |  Val. Acc: 69.27% 

	Train Loss: 0.940 | Train Acc: 83.79%
	 Val. Loss: 1.023 |  Val. Acc: 68.78% 

	Train Loss: 0.938 | Train Acc: 86.21%
	 Val. Loss: 1.022 |  Val. Acc: 69.76% 

	Train Loss: 0.930 | Train Acc: 86.12%
	 Val. Loss: 1.023 |  Val. Acc: 70.73% 

	Train Loss: 0.937 | Train Acc: 85.78%
	 Val. Loss: 1.025 |  Val. Acc: 70.73% 

	Train Loss: 0.933 | Train Acc: 84.91%
	 Val. Loss: 1.015 |  Val. Acc: 70.24% 

	Train Loss: 0.937 | Train Acc: 83.73%
	 Val. Loss: 1.018 |  Val. Acc: 69.27% 

	Train Loss: 0.949 | Train Acc: 81.55%
	 Val. Loss: 1.019 |  Val. Acc: 67.32% 

	Train Loss: 0.930 | Train Acc: 85.43%
	 Val. Loss: 1.016 |  Val. Acc: 67.32% 

	Train Loss: 0.930 | Train Acc: 87.67%
	 Val. Loss: 1.016 |  Val. Acc: 68.29% 

	Train Loss: 0.930 | Train Acc: 87.31%
	 Val. Loss: 1.017 |  Val. Acc: 68.78% 

	Train Loss: 0.922 | Train Acc: 89.05%
	 Val. Loss: 1.017 |  Val. Acc: 68.78% 

	Train Loss: 0.933 | Train Acc: 87.76%
	 Val. Loss: 1.015 |  Val. Acc: 69.27% 

	Train Loss: 0.927 | Train Acc: 88.73%
	 Val. Loss: 1.015 |  Val. Acc: 69.27% 

	Train Loss: 0.930 | Train Acc: 86.72%
	 Val. Loss: 1.016 |  Val. Acc: 69.76% 

	Train Loss: 0.932 | Train Acc: 86.90%
	 Val. Loss: 1.017 |  Val. Acc: 70.24% 

	Train Loss: 0.932 | Train Acc: 85.43%
	 Val. Loss: 1.017 |  Val. Acc: 71.71% 

	Train Loss: 0.932 | Train Acc: 84.66%
	 Val. Loss: 1.019 |  Val. Acc: 71.22% 

	Train Loss: 0.929 | Train Acc: 86.68%
	 Val. Loss: 1.018 |  Val. Acc: 71.22% 

	Train Loss: 0.928 | Train Acc: 87.76%
	 Val. Loss: 1.019 |  Val. Acc: 71.22% 

	Train Loss: 0.934 | Train Acc: 85.78%
	 Val. Loss: 1.018 |  Val. Acc: 71.71% 

	Train Loss: 0.927 | Train Acc: 87.93%
	 Val. Loss: 1.021 |  Val. Acc: 69.76% 

	Train Loss: 0.936 | Train Acc: 85.43%
	 Val. Loss: 1.025 |  Val. Acc: 68.78% 

	Train Loss: 0.935 | Train Acc: 84.81%
	 Val. Loss: 1.023 |  Val. Acc: 68.29% 

	Train Loss: 0.929 | Train Acc: 85.95%
	 Val. Loss: 1.014 |  Val. Acc: 69.76% 

	Train Loss: 0.933 | Train Acc: 85.60%
	 Val. Loss: 1.014 |  Val. Acc: 70.73% 
```

#### Results
  An Validation Accuracy of 70.73% was achieved after 50 epochs.

  
  ![alt text](https://github.com/asravankumar/END2.0/blob/master/session_6/training_loss.png)
  ![alt text](https://github.com/asravankumar/END2.0/blob/master/session_6/valid_loss.png)
  ![alt text](https://github.com/asravankumar/END2.0/blob/master/session_6/training_accuracies.png)
  ![alt text](https://github.com/asravankumar/END2.0/blob/master/session_6/validation_accuracies.png)

#### Intermediate Vectors for an example
  - Tweet: #edshow Whenever Obama tells the truth about the Gop, they boo hoo hoo and call him a bully.

  - Encoder Output after each token step:

```
input word: "#"
encoding output vector: tensor([ 2.2731e-01,  9.0392e-01,  9.8247e-01, -7.6665e-01,  7.3248e-01,
         8.0055e-01,  1.7891e-01, -3.2344e-01, -8.9537e-01,  8.0129e-01,
         5.8254e-01, -9.5525e-02, -7.0017e-01,  9.3375e-02,  9.0207e-01,
        -5.3206e-01,  3.9202e-01, -9.8635e-01, -6.4371e-01,  8.8590e-01,
         7.5047e-01,  5.7631e-02,  9.9099e-02, -2.9164e-01,  9.9769e-01,
        -2.4951e-01,  6.1607e-01,  4.9111e-01,  7.3600e-01,  5.8062e-01,
        -9.7571e-01, -8.8418e-01,  7.8982e-01, -6.6020e-01, -9.8906e-01,
        -7.2674e-01, -7.4862e-01, -1.9327e-01, -7.9938e-01,  7.1821e-01,
        -3.4402e-01,  9.7357e-01,  5.4523e-01, -6.2234e-01, -1.8269e-01,
         6.6918e-01, -3.6602e-01,  5.5157e-01, -1.4819e-01,  9.4262e-01,
        -8.5948e-01,  9.3171e-01, -2.4486e-01, -9.5248e-01,  9.1137e-01,
        -6.7405e-01,  9.7649e-01, -4.2337e-01, -3.0715e-01,  5.8321e-03,
        -9.7495e-01, -6.4421e-01,  5.3289e-01,  4.9357e-01, -5.9938e-01,
        -4.1678e-01, -4.2932e-01,  6.5815e-02, -2.0661e-01, -7.0866e-02,
         7.2567e-01, -2.1234e-01, -6.4525e-01, -9.8545e-01,  8.8169e-01,
        -8.3592e-01, -9.2205e-01, -5.0987e-01,  6.4337e-01,  9.0731e-01,
         2.4663e-01,  6.4383e-01, -5.1719e-01, -2.9724e-01, -3.1427e-04,
        -9.9511e-01, -9.5253e-01, -9.6974e-01,  2.1799e-01, -7.1824e-01,
        -9.2497e-01, -8.7015e-01, -4.9811e-01, -9.1867e-01,  1.1634e-01,
         4.7577e-02,  7.7786e-01, -8.8094e-01,  9.5011e-01,  5.4530e-01],
       grad_fn=<SelectBackward>)
--------
input word: "edshow"
encoding output vector: tensor([-0.3772, -0.0996,  0.9446, -0.8942, -0.0992, -0.0507, -0.8430, -0.0343,
        -0.5274,  0.8560, -0.0731,  0.3505, -0.7927, -0.5013,  0.9870, -0.2290,
        -0.1209, -0.6227, -0.4846, -0.7288,  0.9381,  0.4227, -0.4262, -0.2052,
        -0.0808, -0.8984, -0.9441,  0.8926,  0.4003,  0.7246, -0.8141,  0.0671,
        -0.8908, -0.6890, -0.9293, -0.8281,  0.8188, -0.4353,  0.9412, -0.3038,
        -0.9658, -0.3723, -0.9384, -0.5325, -0.9802,  0.2799, -0.3951, -0.8930,
         0.8571,  0.8368, -0.9576,  0.9622,  0.9603, -0.9895, -0.7264, -0.9822,
        -0.1402, -0.8487, -0.1423, -0.9771, -0.5590, -0.7291,  0.3063,  0.9274,
         0.2022,  0.7395, -0.5528, -0.4792,  0.7499, -0.9994,  0.9111,  0.7282,
        -0.7575, -0.9720,  0.2762, -0.9441, -0.8254,  0.5239,  0.3148,  0.0692,
        -0.9931,  0.1840,  0.5453, -0.9676, -0.9021,  0.8208,  0.8613,  0.9875,
         0.7229,  0.3417,  0.1757,  0.1190, -0.2530, -0.0782, -0.5557, -0.7898,
         0.5803, -0.8215, -0.9692,  0.7426], grad_fn=<SelectBackward>)
--------
input word: "Whenever"
encoding output vector: tensor([-0.7716,  0.4591, -0.7972,  0.1312,  0.4518, -0.4622,  0.1591,  0.8903,
        -0.0152, -0.4701, -0.8092, -0.6287, -0.0792,  0.8248,  0.0040, -0.1612,
        -0.0982, -0.9327, -0.3128,  0.9313,  0.6781,  0.0118,  0.9142, -0.9310,
        -0.1202,  0.5688,  0.2749,  0.0207,  0.7900, -0.8220, -0.0887, -0.1263,
         0.9057,  0.9305,  0.1271,  0.9731, -0.9614,  0.9974, -0.8604,  0.4672,
        -0.6003,  0.4032, -0.8398, -0.8803,  0.9971,  0.8924,  0.1958,  0.7165,
         0.4821, -0.6039,  0.3717,  0.5750,  0.9639, -0.0105, -0.9429, -0.5480,
        -0.9751, -0.9103,  0.7980,  0.9894,  0.9830,  0.8629,  0.2444, -0.0999,
         0.6459,  0.1941,  0.9184,  0.9031,  0.2330, -0.9202, -0.1321,  0.6933,
        -0.9927,  0.9935,  0.8589,  0.9935, -0.2505, -0.4329, -0.2236, -0.7615,
        -0.9469,  0.0378, -0.7995,  0.2884,  0.9494,  0.4348, -0.9839,  0.7447,
         0.9610, -0.4010, -0.4692, -0.4621, -0.8771, -0.4468,  0.9263,  0.9918,
         0.8216, -0.0323,  0.4033,  0.8573], grad_fn=<SelectBackward>)
--------
input word: "Obama"
encoding output vector: tensor([ 0.3954,  0.4821, -0.9275, -0.5659, -0.9939, -0.6432,  0.9615,  0.3354,
         0.7338,  0.1392, -0.0579, -0.7549,  0.9522, -0.8714, -0.9487, -0.1867,
         0.9297,  0.9240,  0.3056,  0.7703,  0.5562, -0.8136,  0.5152, -0.8218,
        -0.9013,  0.8900, -0.9618,  0.1660, -0.9913,  0.8519, -0.9523,  0.9601,
        -0.7312,  0.9064,  0.6815, -0.7087,  0.5252,  0.5981,  0.8636, -0.8882,
         0.6545, -0.7477, -0.3920, -0.7362, -0.9953, -0.4941,  0.6028, -0.9941,
        -0.1648,  0.5879, -0.9921,  0.0943,  0.9614, -0.7165,  0.2923, -0.5954,
        -0.5770,  0.9403,  0.4865,  0.6443, -0.0304,  0.9986, -0.4823,  0.0105,
        -0.2089,  0.0783,  0.9359,  0.1231,  0.0945,  0.9068, -0.9055,  0.9943,
         0.4687, -0.7004,  0.7054,  0.8754,  0.4640,  0.4174, -0.7252,  0.9239,
        -0.0914, -0.2057,  0.2341,  0.0027, -0.1469, -0.3300, -0.7920,  0.9971,
        -0.1199, -0.3992, -0.0165,  0.6145,  0.7651,  0.8383, -0.6235,  0.4583,
        -0.9606,  0.9661, -0.9962,  0.9611], grad_fn=<SelectBackward>)
--------
input word: "tells"
encoding output vector: tensor([ 0.9116,  0.1867, -0.6524,  0.5635, -0.8407,  0.6875,  0.3028, -0.9955,
        -0.5155,  0.5185,  0.7322, -0.7387, -0.2739,  0.5126,  0.8527,  0.9058,
        -0.7865,  0.3428, -0.5275,  0.8277, -0.1250,  0.8831,  0.0529, -0.4742,
         0.9724, -0.5094,  0.1028, -0.5417, -0.8029, -0.9853,  0.8199, -0.9817,
         0.9332,  0.8758, -0.3097,  0.4680, -0.9795,  0.4995, -0.9572, -0.8482,
         0.9719,  0.4587,  0.9710,  0.9018,  0.6997,  0.6798, -0.6477, -0.8801,
         0.9470,  0.8333, -0.4724,  0.5218, -0.7605, -0.4065,  0.5046, -0.8571,
         0.4111,  0.9912, -0.1222,  0.1284, -0.5847, -0.6035,  0.7189, -0.9150,
         0.7242, -0.9832,  0.9413,  0.2670, -0.7343,  0.9331, -0.9520, -0.8167,
         0.4653,  0.9979, -0.7694, -0.6684, -0.9103,  0.4912, -0.7143,  0.5624,
         0.5441,  0.8901, -0.9992,  0.9338,  0.8298,  0.5371, -0.9469,  0.0728,
         0.3020, -0.8469, -0.1924, -0.9626, -0.1184, -0.9084, -0.6629, -0.8533,
         0.6086,  0.6720, -0.4661,  0.9004], grad_fn=<SelectBackward>)
--------
input word: "the"
encoding output vector: tensor([-0.9008, -0.9693,  0.8682,  0.1750,  0.9625, -0.6950, -0.2113,  0.8606,
        -0.9339,  0.8510, -0.4255, -0.1208,  0.8897,  0.9265,  0.8425,  0.4583,
        -0.9680, -0.9200, -0.7474,  0.8435,  0.9875, -0.8625,  0.6046,  0.5633,
        -0.1202, -0.3923,  0.5170, -0.9517,  0.4769, -0.4318,  0.4225,  0.4940,
        -0.7114,  0.9310, -0.7818, -0.6558,  0.6964, -0.1186, -0.9267,  0.8955,
         0.2477, -0.9052, -0.8896, -0.6323, -0.7708, -0.4641,  0.4687,  0.1992,
         0.0774, -0.4088, -0.3055, -0.7252,  0.9736, -0.8185, -0.6100, -0.9551,
         0.9345, -0.5984,  0.9366,  0.8998, -0.6684,  0.2917, -0.8880,  0.3956,
        -0.3778, -0.5888, -0.9351,  0.8510,  0.4036,  0.2653, -0.9683,  0.9738,
         0.9028,  0.9270, -0.0203,  0.8115,  0.9121, -0.5478,  0.0384,  0.1963,
        -0.4745,  0.9172, -0.6689, -0.3861,  0.6386,  0.3897, -0.0615,  0.9798,
         0.7594, -0.9532,  0.4607, -0.6749, -0.6591,  0.9304,  0.6712,  0.9189,
         0.6623,  0.9476,  0.2573, -0.6916], grad_fn=<SelectBackward>)
--------
input word: "truth"
encoding output vector: tensor([ 0.8410, -0.6706,  0.9605,  0.6314,  0.9102,  0.7879, -0.4603, -0.5931,
        -0.3552, -0.1895,  0.3532,  0.8755,  0.9825, -0.8314, -0.0533,  0.5194,
         0.9501,  0.6134,  0.9630,  0.9158,  0.8964, -0.9270,  0.7760, -0.9897,
        -0.5850, -0.7446, -0.6676,  0.8488,  0.9872,  0.9837, -0.8656,  0.6825,
        -0.9397,  0.8290,  0.6237, -0.2030,  0.2087, -0.7487,  0.6358,  0.3355,
        -0.2044, -0.5840,  0.1334,  0.7458, -0.9736, -0.9394,  0.1562,  0.8476,
         0.7776, -0.8216,  0.2011, -0.8816,  0.5742,  0.3950, -0.9812, -0.5082,
         0.9208, -0.7826, -0.0796, -0.4738,  0.9253,  0.9774,  0.9938,  0.4090,
         0.4254,  0.8115,  0.7989,  0.0655,  0.7311, -0.9726, -0.3006, -0.6691,
         0.6282, -0.4287, -0.8365, -0.9826, -0.9288,  0.6755, -0.7789, -0.3052,
         0.7814,  0.9588, -0.9076,  0.9584,  0.3109, -0.7672, -0.2664,  0.8657,
        -0.3632, -0.2515,  0.9592,  0.7714, -0.5046,  0.9919, -0.2444,  0.5763,
        -0.9733,  0.7353,  0.2905, -0.2099], grad_fn=<SelectBackward>)
--------
input word: "about"
encoding output vector: tensor([-0.9381,  0.8665,  0.9983, -0.7894,  0.9788, -0.1435, -0.3523,  0.6928,
         0.8596,  0.1766, -0.9353, -0.3794, -0.5271, -0.6427,  0.8297,  0.5785,
        -0.0186,  0.4310, -0.4417, -0.8708, -0.3652,  0.5933,  0.9692, -0.9982,
         0.2934,  0.4814,  0.7360, -0.1388,  0.7585, -0.6452, -0.7156, -0.1108,
        -0.9989,  0.6546,  0.9687,  0.8284,  0.6532,  0.9428,  0.2857,  0.7776,
        -0.0836,  0.7265, -0.8971, -0.8333,  0.8264, -0.2780,  0.7253,  0.8925,
         0.7784, -0.9956,  0.8721, -0.4856,  0.9322, -0.0256,  0.2648, -0.6244,
        -0.2043,  0.4266, -0.9051, -0.4487,  0.7893, -0.3229,  0.8718, -0.4289,
        -0.9548,  0.8564, -0.5277,  0.2539, -0.7416, -0.6786, -0.9832,  0.2125,
         0.9072, -0.1499,  0.2898, -0.9328, -0.8364, -0.5598,  0.7491,  0.7528,
        -0.6637,  0.9063,  0.1252,  0.8704, -0.7034, -0.9671,  0.7137, -0.4900,
        -0.4429, -0.9163,  0.9125,  0.8981, -0.9668, -0.8477, -0.9933,  0.4314,
        -0.6703,  0.9911,  0.5276, -0.5204], grad_fn=<SelectBackward>)
--------
input word: "the"
encoding output vector: tensor([-0.9236, -0.6057,  0.9598,  0.6609,  0.7774, -0.9523,  0.8131,  0.9813,
        -0.5685, -0.0287, -0.8233,  0.0681,  0.9516, -0.9723,  0.6423, -0.4268,
        -0.8059,  0.3098, -0.1988, -0.0060,  0.9375, -0.9844,  0.7823, -0.4834,
        -0.9795, -0.9142,  0.5256, -0.8184,  0.9559,  0.5753,  0.7921,  0.9501,
        -0.9604,  0.9951, -0.5954, -0.0526,  0.9920,  0.7067, -0.8621,  0.8592,
        -0.8577, -0.9744, -0.9308, -0.4859, -0.9913, -0.2764,  0.1333,  0.0027,
        -0.7738, -0.4588, -0.6609, -0.4984,  0.9946, -0.9037, -0.9817, -0.6177,
         0.7559, -0.1617,  0.7488,  0.8127,  0.5835,  0.9579, -0.1313,  0.2058,
        -0.9718,  0.8277, -0.9670,  0.9923,  0.5469, -0.3119, -0.8748,  0.9736,
         0.9947,  0.9155,  0.1580,  0.7960,  0.7647,  0.6070,  0.9178,  0.8917,
         0.0372, -0.1008,  0.9255,  0.4532,  0.8319, -0.9934,  0.6793,  0.7284,
         0.3464, -0.7747,  0.9424, -0.9027, -0.9358,  0.2753,  0.5356,  0.8445,
         0.6192,  0.9576, -0.0289, -0.9200], grad_fn=<SelectBackward>)
--------
input word: "Gop"
encoding output vector: tensor([ 0.5536, -0.8854, -0.2027, -0.8310, -0.8330, -0.8614,  0.1102, -0.6897,
         0.8765, -0.4052, -0.7975, -0.8454, -0.5692, -0.8608, -0.3757,  0.8645,
         0.8921, -0.3772,  0.1511, -0.9188, -0.9693,  0.9915, -0.4532,  0.9360,
        -0.9096,  0.2164,  0.1554, -0.9979,  0.6004, -0.2290, -0.9955,  0.5660,
        -0.8663,  0.8585, -0.0044,  0.9745,  0.8186,  0.9219,  0.9520, -0.1284,
         0.7999, -0.3463, -0.9561, -0.7545,  0.4422, -0.5343,  0.4827,  0.7969,
        -0.8356,  0.0671,  0.3180,  0.4122,  0.7689,  0.1766, -0.8429, -0.4062,
        -0.9671,  0.3800, -0.9251,  0.5452, -0.9368,  0.9910,  0.8072, -0.9658,
        -0.3693,  0.3264,  0.6816, -0.8948,  0.8794, -0.6185, -0.7970,  0.6185,
         0.8900,  0.9363,  0.8422,  0.2938, -0.8044,  0.9009, -0.9880,  0.2047,
         0.8819,  0.8641,  0.4130, -0.6821, -0.7335, -0.9958, -0.7800,  0.6982,
         0.5030,  0.2154,  0.3081, -0.5935,  0.3937, -0.8337,  0.8853, -0.9777,
         0.7418,  0.7844,  0.8456, -0.7778], grad_fn=<SelectBackward>)
--------
input word: ","
encoding output vector: tensor([-4.4487e-02, -6.8004e-01,  7.3100e-01,  4.0251e-01,  2.1029e-02,
         2.9584e-01,  8.1198e-01, -9.5184e-01,  6.1499e-01,  9.1134e-01,
        -8.0810e-01, -9.4277e-01, -9.5500e-01,  7.3443e-03,  1.5631e-01,
        -4.9261e-01, -5.4660e-01,  4.2547e-01,  9.5171e-01, -3.8527e-01,
         1.6991e-01,  7.4020e-01, -9.0049e-01, -2.5518e-01,  5.9351e-02,
         9.0286e-01, -2.0092e-01, -9.0399e-01, -9.7089e-01,  7.0449e-01,
         6.3773e-01, -7.3093e-01,  4.7682e-01, -6.9804e-01, -5.3203e-01,
         9.3074e-01,  8.5726e-01,  8.4555e-01,  2.8518e-01,  7.5408e-01,
         3.6140e-01, -1.0856e-01,  6.5809e-01,  9.0725e-02,  1.7423e-01,
        -8.7633e-01,  9.5447e-01, -8.0209e-01, -5.3676e-01, -5.8015e-01,
         7.5241e-01, -5.2089e-01, -6.5768e-01, -8.0097e-01, -8.4374e-01,
        -4.1648e-02, -6.4362e-01,  9.8868e-01,  7.7811e-01, -6.2509e-01,
         6.7180e-01,  2.3035e-01, -6.2023e-01, -8.9825e-01,  9.7808e-01,
        -8.0394e-01,  9.8353e-01, -5.2279e-01, -9.8567e-02,  9.7086e-01,
        -8.6358e-01,  2.9830e-01, -6.8420e-01, -7.3257e-01, -9.8442e-01,
         8.4468e-01,  9.7606e-01, -7.5054e-01, -9.9566e-01, -9.0079e-01,
         2.6852e-01, -7.8604e-01,  9.9079e-02, -1.5035e-01, -5.7481e-01,
         8.6753e-01, -8.8881e-01,  3.0122e-01, -7.2191e-01,  3.1428e-01,
        -9.6223e-01, -7.4536e-01, -7.7158e-04, -9.3734e-01,  8.5229e-01,
        -3.7766e-01,  7.6462e-01,  9.3189e-01,  2.4547e-01,  6.4669e-01],
       grad_fn=<SelectBackward>)
--------
input word: "they"
encoding output vector: tensor([ 0.9529,  0.5911,  0.9265,  0.8094,  0.8380, -0.0448,  0.9610,  0.8552,
        -0.8166,  0.7199,  0.9390,  0.7709, -0.1479,  0.7308,  0.2739,  0.4804,
         0.8884, -0.9090,  0.8838, -0.8281,  0.8704,  0.0779, -0.1510,  0.0559,
         0.9581, -0.9046,  0.7586,  0.5026, -0.5836, -0.5610, -0.6820,  0.7879,
        -0.6162, -0.9522,  0.4014,  0.8608, -0.6715,  0.9599,  0.8857,  0.9911,
         0.7024,  0.8959, -0.8302,  0.9693,  0.8345, -0.3180, -0.9956,  0.8993,
         0.6372, -0.1093,  0.9594,  0.5339, -0.7026,  0.1954,  0.9748, -0.1694,
         0.9001, -0.8406,  0.8659, -0.4756, -0.6050, -0.9544, -0.9913, -0.8185,
         0.5408, -0.8368,  0.9002, -0.9126, -0.8967,  0.6319,  0.4724, -0.8415,
         0.9938, -0.5677,  0.9544,  0.5679,  0.1761, -0.8928, -0.7781,  0.3919,
         0.0831,  0.8421,  0.3120, -0.7030, -0.5851,  0.8705,  0.9829,  0.8276,
        -0.9040,  0.7986,  0.5820,  0.8532, -0.7090, -0.0789, -0.5650,  0.8190,
         0.9850, -0.7062,  0.6957,  0.3110], grad_fn=<SelectBackward>)
--------
input word: "boo"
encoding output vector: tensor([ 0.8017,  0.6822,  0.1539, -0.1331,  0.7175,  0.8845,  0.7545,  0.8630,
        -0.4216,  0.1961, -0.8265,  0.2708,  0.8548, -0.2964,  0.0765, -0.7436,
         0.9290, -0.5854,  0.0013,  0.9896,  0.8639, -0.4366,  0.5940, -0.3225,
        -0.6201, -0.2810,  0.9294,  0.5993,  0.9345,  0.8726, -0.8401, -0.8652,
        -0.8998,  0.3116, -0.8365, -0.7777,  0.9144,  0.3526,  0.1763,  0.9979,
        -0.4173,  0.8407,  0.8150, -0.3222,  0.5155,  0.7430, -0.1429,  0.9270,
         0.6943, -0.3591,  0.4701,  0.3048,  0.5858,  0.4924, -0.8180,  0.9571,
        -0.6413, -0.7760, -0.2594, -0.6729,  0.3367, -0.3539,  0.8412,  0.7283,
         0.4109,  0.7566,  0.6952, -0.8049,  0.6248,  0.1116,  0.9887,  0.4811,
        -0.9635,  0.9341,  0.0030,  0.7351,  0.2646,  0.2410,  0.7837,  0.2238,
        -0.9342,  0.5770, -0.3730,  0.0593, -0.9180,  0.7721,  0.9655, -0.9229,
         0.7614,  0.2220,  0.3958, -0.2792, -0.9813, -0.8491, -0.0195, -0.3184,
         0.1271, -0.5615,  0.6674, -0.5646], grad_fn=<SelectBackward>)
--------
input word: "hoo"
encoding output vector: tensor([-0.7641, -0.2046, -0.8015, -0.5177, -0.9719,  0.5291, -0.1142, -0.8679,
         0.9848,  0.4467, -0.7068,  0.1011,  0.5745, -0.9849,  0.9481,  0.4756,
         0.5457,  0.8843,  0.9841, -0.9809, -0.8249, -0.7690,  0.2099, -0.7577,
        -0.9893,  0.3994, -0.9298,  0.5973, -0.7557, -0.2283, -0.3893,  0.9129,
        -0.7763, -0.8996, -0.0610,  0.8939, -0.5225, -0.7365,  0.6500,  0.2678,
        -0.4661,  0.2755, -0.7677, -0.6793, -0.6927,  0.6473,  0.2725, -0.8210,
         0.1735, -0.7909, -0.9132, -0.7611,  0.2029, -0.6151, -0.2194,  0.7716,
         0.7877,  0.9944, -0.8186,  0.6169, -0.9432,  0.6962, -0.8895, -0.6300,
        -0.5836,  0.9710,  0.1087,  0.9598, -0.9298,  0.5292,  0.6267,  0.7957,
         0.7559, -0.8999,  0.6798, -0.6426, -0.9586, -0.5730, -0.5780,  0.9312,
         0.9773, -0.6190,  0.9687, -0.9073, -0.9694, -0.6640,  0.9154,  0.0962,
         0.4001,  0.8635, -0.0680,  0.6925, -0.7901,  0.4177,  0.9451, -0.5116,
         0.2219,  0.7449, -0.2220, -0.6412], grad_fn=<SelectBackward>)
--------
input word: "hoo"
encoding output vector: tensor([ 0.4007, -0.9665,  0.6955, -0.9059, -0.3650,  0.6643,  0.9336, -0.7971,
         0.8070,  0.3851, -0.3704,  0.8951, -0.6135, -0.9200,  0.9886,  0.9923,
         0.0980,  0.8863,  0.9328, -0.8983, -0.9853, -0.8444,  0.6046,  0.7965,
         0.0748,  0.8351,  0.4381, -0.3992,  0.7338,  0.3548,  0.6643,  0.7675,
        -0.4295, -0.9912, -0.8476, -0.7028,  0.1805,  0.5495,  0.5739,  0.8811,
        -0.6981,  0.8141,  0.6789, -0.8111, -0.5063,  0.8323, -0.4293,  0.7200,
         0.9844,  0.0787, -0.2662, -0.2742, -0.3724,  0.9635,  0.5868,  0.4825,
         0.9620,  0.7440,  0.3134, -0.6149, -0.9865, -0.9602, -0.4657,  0.9331,
        -0.8867,  0.3903, -0.3340,  0.3832, -0.7631, -0.0392,  0.7465,  0.2849,
         0.1206, -0.8895,  0.3484,  0.2222,  0.3569,  0.2072, -0.9487, -0.5208,
         0.4131, -0.8477,  0.1810,  0.2355, -0.8589, -0.6953,  0.9699, -0.8611,
         0.9675, -0.6730, -0.5585,  0.6774, -0.9489, -0.4475,  0.2370, -0.8313,
         0.9345, -0.7172,  0.9637,  0.9240], grad_fn=<SelectBackward>)
--------
input word: "and"
encoding output vector: tensor([ 0.6785,  0.5066, -0.0200, -0.2293,  0.3393,  0.8569, -0.7377,  0.4705,
        -0.4353, -0.6976,  0.4311,  0.5088, -0.6064,  0.0017,  0.3490,  0.1506,
        -0.9461,  0.5960,  0.9054, -0.3099, -0.9900,  0.9801,  0.8212, -0.7220,
        -0.7327, -0.0203, -0.0814,  0.7693,  0.0257, -0.9950, -0.3646,  0.7239,
        -0.8883,  0.4657,  0.8283, -0.0227,  0.7499,  0.7733, -0.9165,  0.0711,
        -0.1759, -0.8792, -0.9828,  0.5225, -0.8728,  0.6415,  0.9829, -0.8028,
         0.4974,  0.9356,  0.8154, -0.8200,  0.1554,  0.3703,  0.4781, -0.6707,
         0.3401, -0.8033, -0.4791, -0.9980, -0.8990, -0.8663, -0.3662,  0.4847,
        -0.6963, -0.4656, -0.9707,  0.8038, -0.4898, -0.1494, -0.9086,  0.6151,
         0.8828, -0.5701, -0.6118, -0.1365,  0.9334, -0.4076, -0.7382, -0.6191,
        -0.1907, -0.9226, -0.0912, -0.5171,  0.4207, -0.7096,  0.0145,  0.7738,
        -0.5705,  0.1632,  0.9012, -0.9515, -0.7501, -0.7916,  0.2257,  0.5765,
         0.9876,  0.4971, -0.7470,  0.4409], grad_fn=<SelectBackward>)
--------
input word: "call"
encoding output vector: tensor([-0.7861,  0.9347,  0.8678,  0.7376,  0.1687, -0.8993, -0.5610,  0.9566,
         0.5990, -0.4576, -0.3297,  0.1416, -0.1765, -0.4554, -0.2125,  0.1325,
         0.5323, -0.1992,  0.4931,  0.1819, -0.8968,  0.9295, -0.9179, -0.2231,
         0.6368,  0.1989,  0.7447, -0.6295,  0.9751,  0.8128,  0.3609,  0.2066,
        -0.1473, -0.0894,  0.8259, -0.6785,  0.3893,  0.7598, -0.0205,  0.2254,
        -0.6768, -0.7979, -0.0408,  0.7794, -0.0805, -0.6352, -0.4337, -0.8574,
         0.3044,  0.2857, -0.9027, -0.8602,  0.8183,  0.7108, -0.8742, -0.6081,
         0.8830, -0.9813, -0.7314,  0.2077, -0.3263,  0.8284, -0.8861, -0.8632,
         0.8711, -0.9053,  0.5863,  0.6959,  0.9503, -0.7465,  0.6599,  0.9066,
         0.9346,  0.9847, -0.9771,  0.8201,  0.3393,  0.9994,  0.8394,  0.9919,
         0.4426, -0.9934,  0.7202,  0.4645, -0.7005,  0.9372,  0.5974,  0.9144,
        -0.0052,  0.1580, -0.5937, -0.9272, -0.5893, -0.8964,  0.9287, -0.4530,
         0.9949,  0.9667,  0.3503, -0.7082], grad_fn=<SelectBackward>)
--------
input word: "him"
encoding output vector: tensor([-0.1552,  0.8493,  0.8538,  0.4020,  0.8580,  0.3145,  0.3942, -0.8875,
         0.8727, -0.2976,  0.9654,  0.9859,  0.0380,  0.9276, -0.5500, -0.9084,
         0.4654,  0.4649, -0.7635, -0.8999,  0.2023, -0.2188,  0.9934,  0.1275,
        -0.6410,  0.9051, -0.8449,  0.9564, -0.2545, -0.2823,  0.8175, -0.3096,
        -0.6520, -0.4995,  0.6123, -0.3200, -0.7423, -0.9025, -0.9784, -0.9228,
         0.9222,  0.7869,  0.6525,  0.9641, -0.7603,  0.3275, -0.2994, -0.7338,
         0.8459, -0.3325,  0.7079, -0.7375,  0.0212,  0.9333,  0.3841, -0.9981,
         0.4713,  0.9883, -0.3946,  0.9502,  0.7320, -0.4765, -0.9772, -0.5830,
         0.1881,  0.6320,  0.2616, -0.1638, -0.9169, -0.1981, -0.9638, -0.7424,
         0.0097,  0.9219,  0.2654,  0.6369,  0.2547,  0.6899,  0.4874, -0.7753,
         0.9892,  0.9356, -0.8467,  0.7909, -0.9854,  0.0340,  0.3573, -0.0959,
        -0.9359, -0.9542, -0.7843, -0.9530,  0.9685,  0.8965, -0.9282,  0.4202,
        -0.3083, -0.8720,  0.9118,  0.1371], grad_fn=<SelectBackward>)
--------
input word: "a"
encoding output vector: tensor([ 0.7827, -0.9106, -0.3543,  0.4139,  0.5042, -0.7912,  0.7286, -0.8241,
        -0.2011, -0.8554,  0.4932, -0.4459, -0.1955,  0.3977, -0.9732,  0.8723,
         0.6589,  0.2175, -0.9852,  0.6572,  0.7228, -0.6492, -0.8211,  0.7915,
         0.8994,  0.8786, -0.9093, -0.2022,  0.2464, -0.7758,  0.7920, -0.5978,
        -0.7659,  0.6801, -0.5378,  0.5203,  0.8428, -0.0847, -0.7290,  0.6462,
         0.9684, -0.3053,  0.8677,  0.3730,  0.9475,  0.9651, -0.2633,  0.9522,
         0.3968,  0.9087,  0.9841,  0.5123,  0.3470,  0.9615, -0.8275, -0.5122,
        -0.2852, -0.6889, -0.9812,  0.0596, -0.9716, -0.3563,  0.7826, -0.7460,
         0.4819,  0.9255,  0.8095,  0.9857, -0.2910,  0.9522, -0.8415, -0.0493,
        -0.8902, -0.9598, -0.8344,  0.7175, -0.6016, -0.9314,  0.5472, -0.5880,
        -0.3044,  0.4825,  0.6573,  0.6591, -0.5351, -0.4990, -0.7889,  0.4725,
        -0.7268, -0.3369,  0.9570,  0.9412,  0.7410,  0.6051, -0.9245,  0.0763,
         0.2933,  0.8435,  0.8598,  0.5697], grad_fn=<SelectBackward>)
--------
input word: "bully"
encoding output vector: tensor([ 8.7077e-01,  2.1891e-01,  9.3180e-01,  9.7423e-01, -1.2728e-01,
         8.3465e-01,  9.3088e-01,  8.1104e-01,  3.0071e-01,  8.4038e-01,
        -5.5884e-01, -3.4203e-01,  7.9706e-01,  7.8461e-01, -9.8592e-01,
         3.7137e-01,  9.2554e-04, -2.1710e-01,  5.2519e-01, -8.2822e-01,
         9.9139e-01,  9.3439e-01,  9.0684e-01, -8.5708e-01,  6.7155e-01,
        -1.9767e-01, -4.5426e-01, -4.1621e-01,  6.4618e-01, -5.8720e-01,
        -7.3109e-01,  7.7815e-01, -8.2016e-01,  5.6198e-01, -7.7907e-01,
         2.3085e-01,  2.4523e-01,  9.6641e-01, -2.5900e-01,  3.3318e-01,
         1.3480e-01, -2.4055e-01,  3.2826e-01,  7.7124e-01,  7.6505e-01,
         8.6734e-01,  5.6916e-01,  7.3037e-01,  9.8071e-02,  1.3520e-01,
         3.0396e-01, -2.9117e-01, -2.3244e-01, -6.0835e-01, -1.2475e-01,
         4.1346e-02,  7.1963e-01, -4.1657e-01,  2.4703e-01, -9.2648e-02,
         9.9243e-01,  8.1975e-01, -9.4122e-01,  6.9793e-02, -5.0168e-02,
        -3.0925e-01,  9.1527e-01,  7.6407e-01, -7.5825e-01, -4.3910e-01,
        -6.2175e-01, -6.4231e-01,  2.6997e-01,  8.5326e-01,  4.9526e-01,
         5.4041e-01,  7.6964e-01,  3.2874e-01,  1.4587e-01,  3.0940e-01,
        -8.6205e-01, -8.4810e-01, -6.9560e-01,  8.6468e-01,  8.2508e-01,
         9.4614e-01,  2.8069e-01, -1.9479e-01, -5.9459e-01, -8.4403e-01,
        -7.6192e-01, -5.4014e-01, -1.0645e-01, -8.5289e-01,  3.0837e-01,
         8.7756e-01,  4.8495e-01,  9.5237e-01, -2.7636e-02,  7.9846e-01],
       grad_fn=<SelectBackward>)
--------
input word: "."
encoding output vector: tensor([-0.9559,  0.6420, -0.6015,  0.2630, -0.7700, -0.2949, -0.8028,  0.4226,
        -0.9548, -0.5160, -0.8298, -0.0010, -0.5378, -0.3351,  0.5813, -0.8207,
         0.9595, -0.6984,  0.6882, -0.7042, -0.3956, -0.1938, -0.1593, -0.9898,
        -0.9586, -0.9615,  0.7454, -0.8838, -0.4388,  0.0340,  0.8405, -0.9138,
        -0.1505, -0.5728,  0.7609, -0.0628,  0.0581, -0.4716,  0.6257, -0.7699,
         0.9256, -0.8043, -0.2912,  0.4697, -0.8568,  0.9519,  0.3943, -0.9944,
         0.5833, -0.7360, -0.8924,  0.9709,  0.2620, -0.9634, -0.9578, -0.9825,
         0.1184,  0.6679, -0.6208,  0.6436, -0.9980,  0.7024, -0.7514, -0.3934,
        -0.2292,  0.2110,  0.5054,  0.0677,  0.8891,  0.2595, -0.5103,  0.4018,
         0.9122,  0.3912, -0.3159, -0.8525, -0.8754,  0.8187,  0.8714,  0.9821,
         0.8720, -0.4132,  0.2484, -0.9993, -0.9902,  0.6847,  0.9885,  0.9697,
        -0.2916,  0.7673, -0.1338, -0.8990,  0.9540,  0.5846,  0.0254, -0.5709,
         0.9966,  0.9786, -0.6300,  0.8120], grad_fn=<SelectBackward>)
--------

```


  - The Encoder final state hidden vector which is the single vector for sequence:

single vector: tensor([[[-0.9559,  0.6420, -0.6015,  0.2630, -0.7700, -0.2949, -0.8028,
           0.4226, -0.9548, -0.5160, -0.8298, -0.0010, -0.5378, -0.3351,
           0.5813, -0.8207,  0.9595, -0.6984,  0.6882, -0.7042, -0.3956,
          -0.1938, -0.1593, -0.9898, -0.9586, -0.9615,  0.7454, -0.8838,
          -0.4388,  0.0340,  0.8405, -0.9138, -0.1505, -0.5728,  0.7609,
          -0.0628,  0.0581, -0.4716,  0.6257, -0.7699,  0.9256, -0.8043,
          -0.2912,  0.4697, -0.8568,  0.9519,  0.3943, -0.9944,  0.5833,
          -0.7360, -0.8924,  0.9709,  0.2620, -0.9634, -0.9578, -0.9825,
           0.1184,  0.6679, -0.6208,  0.6436, -0.9980,  0.7024, -0.7514,
          -0.3934, -0.2292,  0.2110,  0.5054,  0.0677,  0.8891,  0.2595,
          -0.5103,  0.4018,  0.9122,  0.3912, -0.3159, -0.8525, -0.8754,
           0.8187,  0.8714,  0.9821,  0.8720, -0.4132,  0.2484, -0.9993,
          -0.9902,  0.6847,  0.9885,  0.9697, -0.2916,  0.7673, -0.1338,
          -0.8990,  0.9540,  0.5846,  0.0254, -0.5709,  0.9966,  0.9786,
          -0.6300,  0.8120]]], grad_fn=<UnsqueezeBackward0>)

single vector shape: torch.Size([1, 1, 100])
-----------------------------------------------
```

  - The above single vector is passed as an input to the first LSTM node.
  - The output of this first LSTM node is passed as input to the next LSTM node.
  - The hidden state and cell state is initialised to zero and is passed to the next LSTM layer.


```
Decoder output after 1st step
decoder_output tensor([[[-0.0185,  0.2574, -0.1439, -0.0485, -0.1185,  0.1076, -0.0859,
           0.0385,  0.3344, -0.0763,  0.0359, -0.1412,  0.0129, -0.0480,
          -0.0760,  0.0960,  0.0460, -0.1004, -0.0682, -0.1156,  0.1110,
           0.1445,  0.0101, -0.0585, -0.0378,  0.0327, -0.2076,  0.0246,
           0.2728,  0.1494, -0.0183,  0.2092,  0.1691,  0.0718,  0.0766,
           0.1981, -0.0494, -0.0129,  0.1134, -0.0512, -0.2455,  0.1082,
          -0.0214, -0.0584, -0.0224, -0.0351,  0.1976, -0.0999, -0.1941,
          -0.0259, -0.2339, -0.0569, -0.1112, -0.2607, -0.0233,  0.0791,
          -0.0648, -0.1281, -0.2467,  0.1283, -0.0146,  0.1007,  0.0924,
          -0.0992, -0.1335,  0.1385,  0.1551,  0.1187, -0.1563, -0.0592,
           0.0246,  0.0782,  0.2780, -0.1286, -0.0099,  0.0169,  0.0831,
          -0.0685, -0.2608, -0.0693,  0.0248, -0.0307, -0.0526,  0.0468,
           0.0321, -0.0338,  0.0288,  0.1309, -0.2698,  0.1986,  0.0422,
          -0.1672,  0.1517,  0.0642, -0.0080,  0.1367,  0.0224, -0.0906,
          -0.0783, -0.0610]]], grad_fn=<TransposeBackward0>)

decoder_output shape: torch.Size([1, 1, 100])
-----------------------------------------------
Decoder output after 2nd step
decoder_output tensor([[[-0.0648,  0.1398, -0.0402,  0.0284, -0.0937,  0.0568, -0.0693,
           0.0754,  0.1805, -0.0607,  0.0132, -0.0845,  0.0034, -0.0015,
          -0.0516,  0.1008,  0.0646, -0.0207, -0.0124, -0.1150,  0.0650,
           0.1386, -0.0553, -0.0217, -0.0208,  0.2238, -0.1898,  0.0541,
           0.2049,  0.1083, -0.0591,  0.1072,  0.0843,  0.0994,  0.1018,
           0.1937, -0.0345, -0.0240,  0.0776, -0.0691, -0.1788,  0.0547,
          -0.0731, -0.0635, -0.0017, -0.0146,  0.1354, -0.0975, -0.0364,
          -0.0676, -0.0579, -0.1305, -0.1289, -0.0152, -0.0072,  0.0714,
           0.0187, -0.2393, -0.1373,  0.0717, -0.0565,  0.0961,  0.1198,
          -0.0281, -0.1364,  0.0683,  0.0826,  0.0859, -0.1032, -0.0652,
           0.0112,  0.0840,  0.0460, -0.1414,  0.0066,  0.0466,  0.0340,
          -0.0433, -0.1184, -0.0929,  0.0556, -0.1121, -0.0106,  0.0519,
           0.0371, -0.0534, -0.0043,  0.0802, -0.1040,  0.0687,  0.0612,
          -0.1211,  0.1055,  0.0254, -0.0048,  0.0854, -0.0404, -0.0399,
          -0.0698, -0.0364]]], grad_fn=<TransposeBackward0>)

decoder_output2 shape: torch.Size([1, 1, 100])
```
